{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add ones to the start of X\n",
    "\n",
    "# y = t0 + t1*x1 = [t0, t1] * [1, x1].transpose = t.transpose * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the closed form solution of the linear model parameters \n",
    "# that minimize the cost function (MSE -> Mean Squared Error) is \n",
    "\n",
    "# t = (X.transpose * X).inverse * X.transpose * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.25772432],\n",
       "       [2.88995228]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0_pred, t1_pred = 4.5, 2.5  # the actual values were 4 and 3 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the equivalent code using Scikit-Learn would be\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.25772432]), array([[2.88995228]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# slope of the cost function MSE = 2/m * X.transpose * (X * t - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eta = 0.1  # learning rate\n",
    "n_iteration = 1000\n",
    "m = 100  # number of instances\n",
    "\n",
    "theta = np.random.randn(2, 1)  # random initialization\n",
    "\n",
    "for _ in range(n_iteration):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.25772432],\n",
       "       [2.88995228]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# That is exactly what the closed form equation found as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem with batch gradient descent is that it uses the entire training set to compute the gradients at each step, which makes it very slow when the training set is large.\n",
    "\n",
    "At the opposite extreme Stochastic Gradient Descent just picks a random instance in the training set at each step and computes the gradients based only on the single instance.\n",
    "\n",
    "This makes the algorithm much faster, but on the other hand due to it's random nature this algorithm is much less\n",
    "regular than Batch Gradient Descent: instead of gently decreasing the cost function bounces up and down, decreasing \n",
    "only on average. Over time it will end up very close to the minimum, but once it gets there it will continue to\n",
    "bounce around, never settling down.\n",
    "\n",
    "This meaning if the cost function is irregular it can actually help the algorithm jump out of local minima. So \n",
    "Stochastic Gradient Descent has a higher chance of finding the global mimimum than Batch Gradient Descent.\n",
    "\n",
    "Though randomness is good to jump out of local optima, but bad because it means that the algorithm can never settle\n",
    "at the mimima. Once solution to this is to gradually reduce the learning rate - this is called Simulated Annealing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "m = 100  # number of instances\n",
    "t0, t1 = 5, 50\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index: random_index + 1]\n",
    "        yi = y[random_index: random_index + 1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.25314782],\n",
       "       [2.86588781]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul\\appPackages\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "       eta0=0.1, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='invscaling', loss='squared_loss', max_iter=None,\n",
       "       n_iter=50, n_iter_no_change=5, penalty=None, power_t=0.25,\n",
       "       random_state=None, shuffle=True, tol=None, validation_fraction=0.1,\n",
       "       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stochastic Gradient Descent with Scikit Learn\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_reg = SGDRegressor(n_iter=50, penalty=None, eta0=0.1)\n",
    "sgd_reg.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.28427177]), array([2.92071237]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3 \n",
    "y = 0.5 * X**2 + X + 2 + np.random.rand(m, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.41039671e-04, 7.07347728e-07])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_poly[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.000841039670700372, 7.073477276917901e-07)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][0], X[0][0]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.42433209]), array([[1.00722588, 0.52674129]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a model performs well on the training data but generalizes poorly according to cross-validation metrics, then the model is OVERFITTING. If the model performs poorly on both, then it is UNDERFITTING. This is one way to tell when a model is too simple or too complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BIAS / VARIANCE TradeOff.\n",
    "\n",
    "A models generalization errors can be attributed to 3 factors.\n",
    "\n",
    "Bias => This is part of the generalization error that is caused due to wrong assumptions about the data, such as assuming that the data is linear when it is actually quadratic. A high bias model is most likely to UNDERFIT the training data.\n",
    "\n",
    "Variance => This part is due to the models sensitivity to small variations in the training data. A model with many degrees of freedom, such as a high-degree polynomial model, is likely to have high variance, and thus is likely to OVERFIT the training data.\n",
    "\n",
    "Irreducible Error => This part is due to the noisiness of the data itself. The only way to reduce this is to clean up the data itself.\n",
    "\n",
    "Increasing a model's COMPLEXITY will typically increase it's VARIANCE and reduce it's BIAS. Conversely, reducing a models COMPLEXITY will increase its BIAS and reduce it's VARIANCE. That is why it is called a tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularizing a model (i.e. constraining it) is a good way to reduce OVERFITTING. The fewer degrees of freedom it has, the harder it will be for a model to overfit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to Regularize a Polynomial model is to reduce the number of polynomial degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a linear model Regularization is typically achieved by constraining the weights of the model's coefficeints.\n",
    "\n",
    "<ul>\n",
    "<li>RIDGE REGRESSION</li>\n",
    "<li>LASSO REGRESSION</li>\n",
    "<li>ELASTIC NET</li>\n",
    "</ul>\n",
    "\n",
    "These are 3 different ways to constrain the weights.\n",
    "\n",
    "RIDGE REGRESSION adds half of the L2-norm of the weight vectors to the cost function.\n",
    "LASSO REGRESSION adds the L1-norm of the weight vectors to the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = 100\n",
    "\n",
    "X = 2 * np.random.rand(m, 1)\n",
    "y = 4 + 3 * X + np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Similar to LinearRegression RidgeRegression can also be solved either using a closed form equation\n",
    "# or using Gradient Descent. The pros and cons are the same.\n",
    "\n",
    "# The Ridge class construction takes an argument solver\n",
    "# If solver = \"cholesky\" then it is solved using closed form equation\n",
    "# If solver = \"sag\" (Stochastic Average Gradient Descent it is a variant of SGD), then it is solved using gradient descent\n",
    "\n",
    "ridge_reg = Ridge(alpha=1, solver=\"cholesky\")  \n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', ridge_reg)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('regressor', Ridge(alpha=1, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='cholesky', tol=0.001))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6771799203175926"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y.ravel(), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_reg = Ridge(alpha=1, solver=\"sag\")  \n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', ridge_reg)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6768960231140183"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X, y.ravel())\n",
    "y_pred = pipeline.predict(X)\n",
    "\n",
    "mean_squared_error(y.ravel(), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul\\appPackages\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.5916342331217557"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_reg = SGDRegressor(penalty=\"l2\")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', ridge_reg)\n",
    "])\n",
    "\n",
    "pipeline.fit(X, y.ravel())\n",
    "y_pred = pipeline.predict(X)\n",
    "\n",
    "mean_squared_error(y.ravel(), y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The penalty hyperparameter sets the type of regularization term to use. Specifying \"l2\" indicates that you want SGD to add a regularization term to the cost function equal to half the norm of the l2-norm of the weight vector: this is simply ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO Regression adds the L1-norms of the weight vectors to the cost function.\n",
    "\n",
    "<b>\n",
    "An important characteristic of Lasso Regression is that it tends to completely eliminate the weights of the least important features.\n",
    "\n",
    "In other words Lasso Regression automatically performs feature selection and outputs a sparse model.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6868737427576502"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', lasso_reg)\n",
    "])\n",
    "\n",
    "pipeline.fit(X, y.ravel())\n",
    "y_pred = pipeline.predict(X)\n",
    "\n",
    "mean_squared_error(y.ravel(), y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELASTIC NET is a middle ground between Ridge and Lasso Reg. The regularization term is simple mix of Ridge and Lasso's regularization terms. You can control the mix using the ratio r. If r = 0 , Elastic net is equivalant to Ridge and if r=1 the Elastic Net is equivalent to Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6942385935096275"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# l1_ratio is correspond to the mix ratio r\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', elastic_net)\n",
    "])\n",
    "\n",
    "pipeline.fit(X, y.ravel())\n",
    "y_pred = pipeline.predict(X)\n",
    "\n",
    "mean_squared_error(y.ravel(), y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic / Logit Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is commonly used to estimate the probability that an instance belongs to a particular class. (e.g. What is the probability that an email is a SPAM ?) If the probability is > 50% then the model predicts that the instance belongs to that class, else the model predicts that it does not. This makes it a BINARY CLASSIFIER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-log(p)    if y = 1 <br>\n",
    "-log(1-p)  if y = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function over the whole training set is the average cost over all the training instances. This can be represented as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J\\left( \\theta  \\right) =  - {1 \\over m}\\sum\\limits_{i = 1}^m {\\left[ {{y^{(i)}}\\log \\left( {{{\\mathop p\\limits^ \\wedge  }^{\\left( i \\right)}}} \\right) + \\left( {1 - {y^{\\left( i \\right)}}} \\right)\\log \\left( {1 - {{\\mathop p\\limits^ \\wedge  }^{\\left( i \\right)}}} \\right)} \\right]} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "$$\\mathop p\\limits^ \\wedge  \\left( i \\right) = {h_\\theta }\\left( x \\right) = \\sigma \\left( {{\\theta ^T}x} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "$$\\sigma \\left( t \\right) = {1 \\over {1 + \\exp \\left( { - t} \\right)}}$$\n",
    "\n",
    "Is the Sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bad news is that there is no known closed form equation to compute the value of theta that minimizes the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the good news is that this cost function is convex, so Gradient Descent or any other optimization algorithm is guranteed to find the global minimum (if the learning rate is not too large and we wait long enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['data'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to build a classifier to detect the iris virginica type based only on the \"petal width\" feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = iris['data'][:, 3:]\n",
    "y = (iris['target'] == 2).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul\\appPackages\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the model's estimated probabilities for for flowers with petal widths varying from 0 to 3 cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e8611177b8>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCOGCpLgLIYQLkuIuhBAuSIq7EEK4oP8HV4TaR4mvJHcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e861117588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_new, y_proba[:, 1], \"g-\", label=\"Iris-Virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", label=\"Not Iris-Virginica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like other Linear Models, Logistic Regression models can be regularized using l1 and l2 penalties. Scikit-Learn actually adds an l2 penalty by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hyperparameter controlling the regularization strength of a Scikit-Learn LogisticRegression model is not alpha (as in other linear models), but its inverse C. The higher the value of C, the less the model is regularized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn's LogisticRegression used the one-vs-all by default when trained on more than 2 classes. , but we can set the <b>multi_class</b> hyperparameter to \"<b>multinomial</b>\" to switch it to Softmax Regression instead. You must also specify a SOLVER that supports Softmax Regression, such as \"<b>lbfgs</b>\" It also applies the l2 regularization by default, which can be controlled using the hyperparameter C. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = iris['data'][:, (2, 3)]  # petal length and petal width\n",
    "y = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10)\n",
    "softmax_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_reg.predict([[5, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.38014896e-07, 5.74929995e-02, 9.42506362e-01]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_reg.predict_proba([[5, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
